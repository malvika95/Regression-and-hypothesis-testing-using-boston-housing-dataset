---
title: 'IMT 573: Problem Set 7 - Regression - Solutions'
author: "Malvika Mohan"
date: 'Due: Tuesday, November 19, 2019'
output: pdf_document
header-includes:
- \newcommand{\benum}{\begin{enumerate}}
- \newcommand{\eenum}{\end{enumerate}}
- \newcommand{\bitem}{\begin{itemize}}
- \newcommand{\eitem}{\end{itemize}}
---

<!-- This syntax can be used to add comments that are ignored during knitting process. -->

##### Collaborators: <!-- BE SURE TO LIST ALL COLLABORATORS HERE! -->

##### Instructions:



##### Setup

In this problem set you will need, at minimum, the following R packages.

```{r Setup, message=FALSE}
# Load standard libraries
library(tidyverse)
library(MASS) # Modern applied statistics functions
```

\textbf{Housing Values in Suburbs of Boston}

In this problem we will use the Boston dataset that is available in the \texttt{MASS} package. This dataset contains information about median house value for 506 neighborhoods in Boston, MA. Load this data and use it to answer the following questions.

\benum
```{r}
data(Boston)
boston_data <- tbl_df(Boston)
head(boston_data)
```
\item Describe the data and variables that are part of the \texttt{Boston} dataset. Tidy data as necessary.
The variables are as follows :
crim - Per capita crime rate by town
zn - proportion of residential land zoned for lots over 25,000 square feet
indus - proportion of non-retail business acres per town
chas - Charles River dummy variable (value is 1 if tract bounds river)
nox - Nitrogen oxide concentration per 10 million
rm - average number of rooms per dwelling
age - propotion of owner occupied units buits before 1940
dis - weighted mean of distances to five Boston employment centres
rad - index of accessibility to radial highways
tax - full-value property-tax rate per \$10,000
ptratio - pupil to teacher ratio by town
black - the proportion of blacks by town
lstat - percentage of lower status of the population
medv - median value of owner-occupied homes 

```{r}
#Removed any NA Values present
boston_data %>%  na.omit()
```
\item Consider this data in context, what is the response variable of interest?
The response variable of interest is median value of owner-occuopied homes(medv).
  
\item For each predictor, fit a simple linear regression model to predict the response. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions. 
There is statistically significant possitive corelation between the average number of rooms and median value of owner occupied homes since the points lie close to the regression line with only a few outliers present.While for the predictor lstat(lower status of the population) there is a significant negative corelation present.
```{r}
#Plotting the best fit regression line for the median value of occupied homes and per capita crime rate
ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$crim))+ geom_point(shape=1) +geom_smooth(method = 'lm') + labs(x="median value of owner-occupied homes",y="Per capita crime rate by town",title = "Variance of median value of occupied homes and per capita crime rate")

#Plotting the best fit regression line for the median value of occupied homes and per capita crime rate
ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$zn))+ geom_point(shape=1) +geom_smooth(method = 'lm') + labs(x="median value of owner-occupied homes",y="proportion of residential land zoned for lots over 25,000 sq.ft.",title = "Variance of median value of occupied homes and residential land zoned")

#Plotting the best fit regression line for the median value of occupied homes and non-retail business acres per town
ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$indus))+ geom_point(shape=1) +geom_smooth(method = 'lm') + 
  labs(x="median value of owner-occupied homes",y="proportion of non-retail business acres per town",title = "Variance of median value of occupied homes and non-retail business acres per town")


ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$nox))+ geom_point(shape=1) +geom_smooth(method = 'lm') + 
  labs(x="median value of owner-occupied homes",y="nitrogen oxides concentration",title = "Variance of median value of occupied homes and nitrogen oxides concentration")

ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$rm))+ geom_point(shape=1) +geom_smooth(method = 'lm') +
  labs(x="median value of owner-occupied homes",y="average number of rooms per dwelling",title = "Variance of median value of occupied homes and average number of rooms per dwelling")

ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$age))+ geom_point(shape=1) +geom_smooth(method = 'lm') + 
  labs(x="median value of owner-occupied homes",y="proportion of owner-occupied units built ",title = "Variance of median value of occupied homes and proportion of owner-occupied units built ")

ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$dis))+ geom_point(shape=1) +geom_smooth(method = 'lm') + 
  labs(x="median value of owner-occupied homes",y="weighted mean of distances to five Boston employment centres ",title = "Variance of median value of occupied homes and distances to five Boston employment centres ")

ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$rad))+ geom_point(shape=1) +geom_smooth(method = 'lm') + 
  labs(x="median value of owner-occupied homes",y="index of accessibility to radial highways ",title = "Variance of median value of occupied homes and index of accessibility to radial highways")

ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$tax))+ geom_point(shape=1) +geom_smooth(method = 'lm') + 
  labs(x="median value of owner-occupied homes",y="full-value property-tax ",title = " median value of occupied homes and full-value property-tax")

ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$ptratio))+ geom_point(shape=1) +geom_smooth(method = 'lm') + 
  labs(x="median value of owner-occupied homes",y="pupil-teacher ratio by town",title = "Variance of median value of occupied homes and pupil-teacher ratio by town")

ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$black))+ geom_point(shape=1) +geom_smooth(method = 'lm') + 
  labs(x="median value of owner-occupied homes",y="proportion of blacks by town",title = "Variance of median value of occupied homes and proportion of blacks")

ggplot(boston_data,aes(x=boston_data$medv,y=boston_data$lstat))+ geom_point(shape=1) +geom_smooth(method = 'lm') + 

#Calculating co-relation between the our predictor and response variables
bivrel <- cor(boston_data,y=boston_data$medv,use = "everything",method = "pearson") 
bivrel


```

\item Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $H_0: \beta_j = 0$?
On formulating the regression model, I obtained the p value of all the predectors with respect to our response variable(Murder. For all the predictors other than age (owner-occupied units built prior to 1940) and zn (residential land zoned )  we get our p-values to be less than 0.05 and hence we can reject the null hypothesis for them (crim,indus,chas,nox,rm,dis,rad,tax,pratio,black,lstat).            
```{r}

#Storing the predictor variables in a Dummy variable
predictor_var <- subset(boston_data,select = c('crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','black','lstat'))

#Finding the regression model on the median value variable with all the predictors
fit_reg <- lm(boston_data$medv ~.,predictor_var)
summary(fit_reg)



```

\item How do your results from (3) compare to your results from (4)? Create a plot displaying the univariate regression coefficients from (3) on the x-axis and the multiple regression coefficients from part (4) on the y-axis. Use this visualization to support your response.
In question three, I determined the colinearity of each variable with our response variable and all the values were statistically significant. However on plotting a multivariate regression we found two values to be statistically insignificant (age and indus).
```{r}
#Note : I was unable to sepearate the points of the prediction variables while plotting the graph
coefs <- data.frame("predictor"=character(0), "Estimate"=numeric(0), "Std.Error"=numeric(0), "t.value"=numeric(0), "Pr.t"=numeric(0), "r.squared"=numeric(0), stringsAsFactors = FALSE)
j <- 1
for(i in names(boston_data)){
  if(i != "medv"){
    #Finding the multivariable coeefficients and storing them in a coefficient matrix
    fit_reg <- summary(lm(medv ~ eval(parse(text=i)), data=boston_data))
    coefs[j,] <- c(i, fit_reg$coefficients[2,], fit_reg$r.squared)
    j <- j+1
  }
}
#Converted all coefficients to numeric values
coefs[,-1] <- lapply(coefs[,-1], FUN=function(x) as.numeric(x))

fit_reg <- lm(boston_data$medv ~.,boston_data)
#Creating a data frame of all the
df = data.frame("multiple"=summary(fit_reg)$coefficients[-1,1])
df$simple <- NA
for(i in row.names(df)){
  #Removed the nox variable as it is displaced with respect to other points on the graph
  if(!(i %in% "nox" ))
  {
  df[row.names(df)==i, "simplecoeff"] = coefs[coefs[,1]==i, "Estimate"]
}
}
plot(df$simplecoeff , df$multiple, xlab="Coefficients for Simple Linear Regression", ylab="Coeficients for Multiple Linear Regression") + xlim(500,10000) +ylim(10000,2000)
text(x=df$simplecoeff, y=df$multiple, labels=row.names(df), cex=.7, col="blue", pos=4)
```


\item Is there evidence of a non-linear association between any of the predictors and the response? To answer this question, for each predictor $X$ fit a model of the form:
  
  $$ Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon $$

For the variables crim,zn,indus,rm,dis,rad and lstat there is evidence of non linear relationship as the squared and cubed terms of these variables is found to be statistically signficant(p value greater than 0.05).
age also appears to have a non linear relationship when it is cubed as it becomes insignificant.
similarly when nox is squared it appears to have a non linear relationship.
For the rest of the variables there is no evidence of non linear association.

Note :On determining the polynomial coefficients for the predictor chas and our response variable we get the second and third squared values of the polynomial as NA and hence I did not use it in the polynomial regression calculation.


```{r}

#On finding the association between the chas predictor and our response variable (as seen below) we get NA values hence we will not consider it while calculating the non linear association 

lm(medv ~ chas + I(chas^2) + I(chas^3), data = boston_data)

#Storing all predictor variables in a vector
predictor_var <- subset(boston_data,select = c('crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','black','lstat'))
#Creating a data frame with the coefficient details and their types
polynomial_data <- data.frame("predictor"=character(0), "Estimate"=numeric(0), "Standard Error"=numeric(0), "tvalue"=numeric(0), "Pr(t)"=numeric(0), stringsAsFactors = FALSE)

k <- 1
#iterating over the predictor varibles
for(i in names(predictor_var)){
 if(!(i %in% "chas")){
#Evaluating the regression model for each of the predictors with our response variable
       print(paste0('For predictor variable : ',i))
    fit_reg <- summary(lm(medv ~ poly(eval(parse(text=i)),3), data=boston_data))
    print(fit_reg)
   
  }
}



```

\item Consider performing a stepwise model selection procedure to determine the best fit model. Discuss your results. How is this model different from the model in (4)?
The best fit model comes from taking the subset of predictors in the order as :
medv ~ lstat + rm + ptratio + dis + nox + chas + black + zn + 
    crim + rad + tax

We get the final AIC value as 1585.76.
This model is different from model in question 4 as previously we just determined the sigificance values of our response variable (medv) with each of the predictors. However in the step wise best fit model we are finding the subset of predictors that results in a model that lowers prediction errors.It also differes from model in question 4 as we initially start with no predictors and we sequentially add the most contributive predictors and remove any variable that no longer provide an improvement in the model fit until we reach a model with the best fit. 
```{r}
#Reference - http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/
lower_reg <- lm(medv ~ 1, data = boston_data)
upper_reg <- lm(medv ~ ., data = boston_data)

#performing forward step model till all predictors are taken
step_model <- stepAIC(lower_reg , scope = list(lower = lower_reg , upper = upper_reg), direction = "both")



```
\item Evaluate the statistical assumptions in your regression analysis from (7) by performing a basic analysis of model residuals and any unusual observations. Discuss any concerns you have about your model.
References : https://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/
Firstly multiple linear regression requires the relationship between the independent and dependent variables to be linear while this may not be the case as we can see the points are spread across our regression line. Second, the multiple linear regression analysis requires that the errors between observed and predicted values should be normally distributed. Thirdly, our multiple linear regression assumes that there is no multicollinearity in the data.
One concern I have with my model is that even though we see a strong significance and co-relation between some predictor variables with the response variable this may not always be true in reality.

```{r}
residual <- resid(step_model)
plotResiduals <- ggplot(data = data.frame(x = boston_data$medv, y = residual), aes(x = x, y = y)) +
geom_point(color = 'blue', size = 1) + stat_smooth(method='lm',se=FALSE,color='red')+labs(title = 'Residual of multiple regression', x="median value of occupied homes",y='Residuals') 
plotResiduals
```

\eenum


